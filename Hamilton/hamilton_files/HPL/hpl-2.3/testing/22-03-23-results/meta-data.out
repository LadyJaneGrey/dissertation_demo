HPLinpack benchmark input file
Innovative Computing Laboratory, University of Tennessee
HPL.out      output file name (if any) 
6            device out (6=stdout,7=stderr,file)
1            # of problems sizes (N)
400	     Ns
1            # of NBs
192          NBs
0            PMAP process mapping (0=Row-,1=Column-major)
1            # of process grids (P x Q)
1            Ps
1            Qs
16.0         threshold
1            # of panel fact
2            PFACTs (0=left, 1=Crout, 2=Right)
1            # of recursive stopping criterium
4            NBMINs (>= 1)
1            # of panels in recursion
2            NDIVs
1            # of recursive panel fact.
1            RFACTs (0=left, 1=Crout, 2=Right)
1            # of broadcast
1            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
1            # of lookahead depth
1            DEPTHs (>=0)
2            SWAP (0=bin-exch,1=long,2=mix)
64           swapping threshold
0            L1 in (0=transposed,1=no-transposed) form
0            U  in (0=transposed,1=no-transposed) form
1            Equilibration (0=no,1=yes)
8            memory alignment in double (> 0)

#!/bin/bash 

# -- Email Details

#SBATCH --mail-user=rohan.jain@durham.ac.uk
#SBATCH --mail-type=ALL


# -- Processing Specs -- 

# -N : number of nodes
# -n : number of cores (this is intotal not just per core)
# -c: number of mpi ranks per core (hardware threads?)
#Â -ntasks-per-socket = 64 cores per socket

# ---

# -n: number of mpiranks 
# -c: number of cores per mpirank
# -N: number of Nodes





#--#### SBATCH -p multi
#--SBATCH -N 1
#SBATCH -n 1
#SBATCH -c 1
#SBATCH -t 01:00:00
#--SBATCH --ntasks-per-socket=64
#SBATCH --mem=5GB

# Filename

file_name=core2Node1.out

# Running Command
mpirun --allow-run-as-root  -np 1 ./xhpl > ${file_name}




