HPCG-Benchmark version: 3.0
Release date: November 11, 2015
Machine Summary: 
  Distributed Processes: 8
  Threads per processes: 16
Global Problem Dimensions: 
  Global nx: 256
  Global ny: 256
  Global nz: 256
Processor Dimensions: 
  npx: 2
  npy: 2
  npz: 2
Local Domain Dimensions: 
  nx: 128
  ny: 128
  nz: 128
########## Problem Summary  ##########: 
Setup Information: 
  Setup Time: 5.89153
Linear System Information: 
  Number of Equations: 16777216
  Number of Nonzero Terms: 449455096
Multigrid Information: 
  Number of coarse grid levels: 3
  Coarse Grids: 
    Grid Level: 1
    Number of Equations: 2097152
    Number of Nonzero Terms: 55742968
    Number of Presmoother Steps: 1
    Number of Postsmoother Steps: 1
    Grid Level: 2
    Number of Equations: 262144
    Number of Nonzero Terms: 6859000
    Number of Presmoother Steps: 1
    Number of Postsmoother Steps: 1
    Grid Level: 3
    Number of Equations: 32768
    Number of Nonzero Terms: 830584
    Number of Presmoother Steps: 1
    Number of Postsmoother Steps: 1
########## Memory Use Summary  ##########: 
Memory Use Information: 
  Total memory used for data (Gbytes): 11.9991
  Memory used for OptimizeProblem data (Gbytes): 0
  Bytes per equation (Total memory / Number of Equations): 715.205
  Memory used for linear system and CG (Gbytes): 10.5592
  Coarse Grids: 
    Grid Level: 1
    Memory used: 1.26214
    Grid Level: 2
    Memory used: 0.157993
    Grid Level: 3
    Memory used: 0.0198084
########## V&V Testing Summary  ##########: 
Spectral Convergence Tests: 
  Result: PASSED
  Unpreconditioned: 
    Maximum iteration count: 11
    Expected iteration count: 12
  Preconditioned: 
    Maximum iteration count: 2
    Expected iteration count: 2
Departure from Symmetry |x'Ay-y'Ax|/(2*||x||*||A||*||y||)/epsilon: 
  Result: PASSED
  Departure for SpMV: 7.89383e-11
  Departure for MG: 5.26255e-11
########## Iterations Summary  ##########: 
Iteration Count Information: 
  Result: PASSED
  Reference CG iterations per set: 50
  Optimized CG iterations per set: 50
  Total number of reference iterations: 50
  Total number of optimized iterations: 50
########## Reproducibility Summary  ##########: 
Reproducibility Information: 
  Result: PASSED
  Scaled residual mean: 0.0011907
  Scaled residual variance: 0
########## Performance Summary (times in sec) ##########: 
Benchmark Time Summary: 
  Optimization phase: 6.79865e-07
  DDOT: 5.2684
  WAXPBY: 0.0746081
  SpMV: 1.70267
  MG: 47.4
  Total: 54.4469
Floating Point Operations Summary: 
  Raw DDOT: 5.06672e+09
  Raw WAXPBY: 5.06672e+09
  Raw SpMV: 4.58444e+10
  Raw MG: 2.56195e+11
  Total: 3.12173e+11
  Total with convergence overhead: 3.12173e+11
GB/s Summary: 
  Raw Read B/W: 35.3211
  Raw Write B/W: 8.16254
  Raw Total B/W: 43.4836
  Total with convergence and optimization phase overhead: 43.0181
GFLOP/s Summary: 
  Raw DDOT: 0.961719
  Raw WAXPBY: 67.9112
  Raw SpMV: 26.9251
  Raw MG: 5.40495
  Raw Total: 5.73352
  Total with convergence overhead: 5.73352
  Total with convergence and optimization phase overhead: 5.67214
User Optimization Overheads: 
  Optimization phase time (sec): 6.79865e-07
  Optimization phase time vs reference SpMV+MG time: 6.10779e-07
DDOT Timing Variations: 
  Min DDOT MPI_Allreduce time: 0.602487
  Max DDOT MPI_Allreduce time: 5.17125
  Avg DDOT MPI_Allreduce time: 1.30209
__________ Final Summary __________: 
  HPCG result is VALID with a GFLOP/s rating of: 5.67214
      HPCG 2.4 Rating (for historical value) is: 5.73352
  Reference version of ComputeDotProduct used: Performance results are most likely suboptimal
  Reference version of ComputeSPMV used: Performance results are most likely suboptimal
  Reference version of ComputeMG used and number of threads greater than 1: Performance results are severely suboptimal
  Reference version of ComputeWAXPBY used: Performance results are most likely suboptimal
  Results are valid but execution time (sec) is: 54.4469
       You have selected the QuickPath option: Results are official for legacy installed systems with confirmation from the HPCG Benchmark leaders.
       After confirmation please upload results from the YAML file contents to: http://hpcg-benchmark.org
